{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Dict approach\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "#from typing import Any, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load genome loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load genome loader for salk cluster\n",
    "sys.path.append('/iblm/netapp/home/jjaureguy/genome_loader/genome-loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load genome loader for runAI cluster\n",
    "sys.path.append('/home/jovyan/home/jjaureguy/genome_loader/genome-loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import genome_loader.write_h5\n",
    "import genome_loader.encode_data\n",
    "import genome_loader.get_data\n",
    "import genome_loader.get_encoded\n",
    "import genome_loader.load_data\n",
    "import genome_loader.load_h5\n",
    "\n",
    "from genome_loader.write_h5 import write_encoded_genome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in dataframes for Train/valid/test on salk cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Dataset not split \n",
    "#unified_bed_file_df = pd.read_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/unified_bed_df.txt',sep='\\t')\n",
    "#frag_tn5_h5_df = pd.read_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/frag_tn5_h5_df.txt',sep='\\t')\n",
    "\n",
    "#OHE for AGCT\n",
    "one_hot_enc_genome = h5py.File('/iblm/netapp/data4/jjaureguy/out_dir/genome_onehot.h5','r')\n",
    "\n",
    "# Train\n",
    "frag_tn5_train_df = pd.read_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/salk_df/train_frag_tn5_h5_df.txt',sep='\\t')\n",
    "train_bed_file_df = pd.read_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/salk_df/train_peak_df_filtered.txt',sep='\\t')\n",
    "\n",
    "# Validation\n",
    "frag_tn5_valid_df = pd.read_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/salk_df/valid_frag_tn5_h5_df.txt',sep='\\t')\n",
    "valid_bed_file_df = pd.read_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/salk_df/valid_peak_df_filtered.txt',sep='\\t')\n",
    "# Test\n",
    "frag_tn5_test_df = pd.read_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/salk_df/test_frag_tn5_h5_df.txt',sep='\\t')\n",
    "test_bed_file_df = pd.read_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/salk_df/test_peak_df_filtered.txt',sep='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in dataframes for Train/valid/test on runAI cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OHE for AGCT\n",
    "one_hot_enc_genome = h5py.File('/home/jovyan/data4/jjaureguy/out_dir/genome_onehot.h5','r')\n",
    "# Train\n",
    "\n",
    "frag_tn5_train_df = pd.read_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/runai_df/train_frag_tn5_h5_df_runai.txt',sep='\\t')\n",
    "train_bed_file_df = pd.read_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/runai_df/train_peak_df_filtered.txt',sep='\\t')\n",
    "\n",
    "# Validation\n",
    "frag_tn5_valid_df = pd.read_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/runai_df/valid_frag_tn5_h5_df_runai.txt',sep='\\t')\n",
    "valid_bed_file_df = pd.read_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/runai_df/valid_peak_df_filtered.txt',sep='\\t')\n",
    "\n",
    "# Test\n",
    "frag_tn5_test_df = pd.read_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/runai_df/test_frag_tn5_h5_df_runai.txt',sep='\\t')\n",
    "test_bed_file_df = pd.read_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/runai_df/test_peak_df_filtered.txt',sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Custom Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "#from .transforms import KmerShuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tester Code for dataset class experimental code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# H5 files function from class\n",
    "\n",
    "# Empty dict for h5 file paths and h5 files tn5 inserttion cuts\n",
    "h5_files = {}\n",
    "def get_h5(h5_path):\n",
    "        # checks if h5 path is in h5 files\n",
    "        if h5_path in h5_files:\n",
    "            return h5_files[h5_path]\n",
    "        else:\n",
    "            # key, values dictionary\n",
    "            h5_files[h5_path] = h5py.File(h5_path, \"r\")\n",
    "\n",
    "#Function for selecting 1024 bp window of X features(one hot encoded ref genome peak range)\n",
    "def get_mid_point(start, end):\n",
    "    np.random.seed(0)\n",
    "    mid_point = range(start,end)\n",
    "    mid_point = np.random.choice(mid_point, 1, replace=True)\n",
    "    #print('mid pt selection random', (mid_point))\n",
    "    mid_point = mid_point.item()\n",
    "    #print(mid_point)\n",
    "    return mid_point\n",
    "\n",
    "def get_x_window(mid_point):\n",
    "    #1) pick random point in range_num(list) and set to variable\n",
    "    #2) Check that the point is within the bounds of +/- 512 of end of OHE chrom boundary \n",
    "    #3) Create window of 1024 window around point \n",
    "    bounds = range(mid_point-512, mid_point + 512)\n",
    "    # \n",
    "    mid_point = list(bounds)\n",
    "\n",
    "    return mid_point\n",
    "\n",
    "def get_y_window(mid_point):\n",
    "    # same thing but without check since 256 bp window is within the 1024bp window\n",
    "    mid_point = list(range(mid_point-128, mid_point + 128))\n",
    "    return mid_point\n",
    "        \n",
    "# T = total_tn5_counts_per_chrom\n",
    "# C = Counts per window(summed 256 bp region)\n",
    "def normalize_atac_reads(Tn5_counts_window, T):\n",
    "    counts = []\n",
    "    #******Figure out how to iterate over the multiple dimension array and calucate the normaliztion....\n",
    "    for i in range(0,len(Tn5_counts_window)):\n",
    "        C = sum(Tn5_counts_window[i])\n",
    "        print('C',C)\n",
    "        length = len(Tn5_counts_window[i])\n",
    "        print('length',length)\n",
    "        norm = (C*10**9/(T*(length)))\n",
    "        norm = round(norm)\n",
    "        counts.append(norm)\n",
    "    return counts\n",
    "def bin_window(window,  bin_size):\n",
    "        length_window = len(window)\n",
    "        bins = []\n",
    "        for i in range(0, length_window, bin_size):\n",
    "            bins.append((window[i:i+bin_size]))\n",
    "        print(bins)\n",
    "        return bins\n",
    "    \n",
    "# Dummy index\n",
    "index = range(1,10)\n",
    "print(index)\n",
    "# bam file index \n",
    "for i in index:\n",
    "    bam_file_index = (index[i] % len(frag_tn5_valid_df))\n",
    "    # Peak index \n",
    "    peak_index = (index[i] // len(frag_tn5_valid_df))\n",
    "    # Coordinates for unified bed fil \n",
    "    coords = valid_bed_file_df.iloc[peak_index] \n",
    "\n",
    "    #Older code to revert back to \n",
    "    # Start end range\n",
    "    #start_end = range(coords.start,coords.end)\n",
    "    #print('start and end', start_end)\n",
    "    # valid_bed_file_df[coords.start:coords.end]\n",
    "\n",
    "    window = (get_mid_point(coords.start,coords.end))\n",
    "    #print(coords.start,coords.end)\n",
    "    #print(window)\n",
    "\n",
    "\n",
    "\n",
    "    # X Feature(one hot encoded genome for chromosome number of one hot enc h5py file and start end postions)\n",
    "    x =  one_hot_enc_genome[coords.chrom]['onehot'][get_x_window(window)]\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    # if x.all()==False:\n",
    "    #     print(x.all())\n",
    "    #     print('hello')\n",
    "#     for i in range(len(x)):\n",
    "#         if x.all() < 1:\n",
    "#             counter +=1\n",
    "#     if counter >0:\n",
    "#         #print('contains Ns')\n",
    "#         # then we discard and move on \n",
    "#        # hd5_tn_test['chr1']['depth'][()]>0\n",
    "#         continue\n",
    "    \n",
    "#     elif counter==0:\n",
    "#         print('true')\n",
    "#         print(coords.start,coords.end)\n",
    "#         print(window)\n",
    "\n",
    "\n",
    "\n",
    "    # call geth5 to populate h5 dictionary\n",
    "    get_h5(frag_tn5_valid_df.iloc[ bam_file_index,frag_tn5_valid_df.columns.get_loc('diff_frag_h5_path')])\n",
    "    get_h5(frag_tn5_valid_df.iloc[ bam_file_index,frag_tn5_valid_df.columns.get_loc('int_gamma_diff_frag_h5_path')])\n",
    "    get_h5(frag_tn5_valid_df.iloc[ bam_file_index,frag_tn5_valid_df.columns.get_loc('salm_int_gamma_diff_frag_h5_path')])\n",
    "    get_h5(frag_tn5_valid_df.iloc[ bam_file_index,frag_tn5_valid_df.columns.get_loc('salm_diff_frag_h5_path')])\n",
    "    #print(h5_files)\n",
    "\n",
    "    #h5_files\n",
    "    # we need this to be dynamic, path is hardcoded\n",
    "    #y = h5_files.get('/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/10_genos_fastq/HPSI0114i-eipl_1/out/treatments/int_gamma_mb/frag_depths.h5')\n",
    "\n",
    "    \n",
    "    h5_tn5_read_count_diff = h5_files.get(frag_tn5_valid_df.iloc[ bam_file_index,frag_tn5_valid_df.columns.get_loc('diff_frag_h5_path')])\n",
    "    h5_tn5_read_count_int_gamma = h5_files.get(frag_tn5_valid_df.iloc[ bam_file_index,frag_tn5_valid_df.columns.get_loc('int_gamma_diff_frag_h5_path')])\n",
    "    h5_tn5_read_count_salm_int_gamma = h5_files.get(frag_tn5_valid_df.iloc[ bam_file_index,frag_tn5_valid_df.columns.get_loc('salm_int_gamma_diff_frag_h5_path')])\n",
    "    h5_tn5_read_count_salm_diff = h5_files.get(frag_tn5_valid_df.iloc[ bam_file_index,frag_tn5_valid_df.columns.get_loc('salm_diff_frag_h5_path')])\n",
    "    \n",
    "    y_1  = (h5_tn5_read_count_diff[coords.chrom]['depth'][get_y_window(window)])\n",
    "    y_1 = bin_window(window=y_1,bin_size=128)\n",
    "    #print(len(y_1))\n",
    "    T_1 = h5_tn5_read_count_diff.attrs[\"total_sum\"]\n",
    "    #print('T 1',T)\n",
    "    norm_1 = normalize_atac_reads(y_1, T_1)\n",
    "    print('normal', normal)\n",
    "    \n",
    "    y_2 = (h5_tn5_read_count_int_gamma[coords.chrom]['depth'][get_y_window(window)])\n",
    "    y_2 = bin_window(window=y_2,bin_size=128)\n",
    "    T_2 = h5_tn5_read_count_int_gamma.attrs[\"total_sum\"]\n",
    "    norm_2 = normalize_atac_reads(y_2, T_2)\n",
    " \n",
    "    y_3 = (h5_tn5_read_count_salm_int_gamma[coords.chrom]['depth'][get_y_window(window)])\n",
    "    y_3 = bin_window(window=y_3,bin_size=128)\n",
    "    T_3 = h5_tn5_read_count_salm_int_gamma.attrs[\"total_sum\"]\n",
    "    norm_3 = normalize_atac_reads(y_3, T_3)\n",
    "        \n",
    "    y_4 = (h5_tn5_read_count_salm_diff[coords.chrom]['depth'][get_y_window(window)])\n",
    "    y_4 = bin_window(window=y_4,bin_size=128)\n",
    "    T_4 = h5_tn5_read_count_salm_diff.attrs[\"total_sum\"]\n",
    "    norm_4 = normalize_atac_reads(y_4, T_4)\n",
    "    \n",
    "    #print(type(y_1))\n",
    "    #y = np.concatenate((y_1,y_2,y_3,y_4))\n",
    "    norm_total = np.stack((norm_1,norm_2,norm_3,norm_4))\n",
    "    # print('len x', len(x))\n",
    "    # print('len y', len(y))\n",
    "    #print(x.shape,y_1.shape,y_2.shape,y_3.shape,y_4.shape)\n",
    "    #print(x.shape, y.shape)\n",
    "    \n",
    "    print(len(x))\n",
    "    print(type(x))\n",
    "    print(x.shape)\n",
    "    print(x.ndim)\n",
    "    # if np.any(x>0):\n",
    "    #     print('true')\n",
    "    # else:\n",
    "    #     print('false')\n",
    "    # counter = 0\n",
    "    # for i in np.nditer((x)):\n",
    "    #     print(i)\n",
    "    #     #x.all(i):\n",
    "#     v = np.array([0,0,0,0])\n",
    "#     print(np.any(np.all(np.isin(x,v,True),axis=1)))\n",
    "    \n",
    "#     # Get a flattened 1D view of 2D numpy array\n",
    "#     flatten_arr = np.ravel(x)\n",
    "    # Check if all value in 2D array are equal\n",
    "    if ([0,0,0,0] == x).all(axis = 1).any():\n",
    "        print('true')\n",
    "    else: \n",
    "        print(x)\n",
    "        print('false')\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class Dataset_class(Dataset):\n",
    "    \"\"\"Dataset_class Dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    one_hot_enc_genome object: pass the H5py file object. One-hot encoded genome `.h5` file. e.g. from genome-loader.\n",
    "    unified_bed_file_df : pass the df object. DF of bed file containing unified peak set of (1st 5 patients). Chrom, start, end.  \n",
    "    frag_tn5_h5_df: pass the df object. DF containing H5py paths for Tn5 read counts. Patient ID, treatment type, H5py path column containg Tn5 read counts\n",
    "    \"\"\"\n",
    "    # class constructor(initialize ohe, bed file, frag df file)\n",
    "    def __init__(self, one_hot_enc_genome, unified_bed_file_df, frag_tn5_h5_df,bin_size, dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.bin_size = bin_size\n",
    "        self.one_hot_enc_genome = one_hot_enc_genome\n",
    "        self.unified_bed_file_df = unified_bed_file_df\n",
    "        self.frag_tn5_h5_df_1 = frag_tn5_h5_df\n",
    "        self.frag_tn5_h5_df_2 = frag_tn5_h5_df\n",
    "        self.frag_tn5_h5_df_3 = frag_tn5_h5_df\n",
    "        self.frag_tn5_h5_df_4 = frag_tn5_h5_df\n",
    "        self.dtype = dtype\n",
    "        # Empty dictionary for h5 file paths\n",
    "        self.h5_files = {}\n",
    "        self.d = defaultdict(list)\n",
    "\n",
    "    # Takes in h5 path from Tn5 from frag_tn5_h5_df column containing h5 paths of Tn5 counts\n",
    "    def get_h5(self, h5_path):\n",
    "        # checks if h5 path is already in dictionary of h5py file paths\n",
    "        if h5_path in self.h5_files:\n",
    "            # returns them if there already there\n",
    "            return self.h5_files[h5_path]\n",
    "        else:\n",
    "            # Sets the path as a key, and the value as the read file\n",
    "            self.h5_files[h5_path] = h5py.File(h5_path, \"r\")\n",
    "            \n",
    "    \"\"\"Function: selects random midpoint from X features(one hot encoded ref genome peak range)\n",
    "          1) pick random point in range_num(list) and set to variable\n",
    "    \"\"\"\n",
    "    def get_mid_point(self, start, end):\n",
    "        #np.random.seed(0)\n",
    "        mid_point = int((start+end)/2)\n",
    "        #mid_point = np.random.choice(mid_point, 1, replace=True)\n",
    "        #print('mid pt selection random', (mid_point))\n",
    "        #mid_point = mid_point.item()\n",
    "        return mid_point\n",
    "    \n",
    "    \n",
    "    \"\"\"Function: selects 1024 bp window of X features(one hot encoded ref genome peak range)\n",
    "          1) utilizes mid point from get_mid_point\n",
    "          2) creates a 1024 window size around midpoint\n",
    "    \"\"\"\n",
    "    def get_x_window(self,mid_point):\n",
    "        bounds = (mid_point-512, mid_point + 512)\n",
    "        self.d['1024_bp_start_end'] = bounds\n",
    "        bounds = range(mid_point-512, mid_point + 512)\n",
    "        mid_point = list(bounds)\n",
    "        return mid_point\n",
    "    \n",
    "    \"\"\"Function: selects 256 bp window of Y features(Tn5 frag counts from h5py file)\n",
    "          1) utilize sames mid point from get_mid_point\n",
    "          2) creates a 256 window size around midpoint\n",
    "    \"\"\"\n",
    "    def get_y_window(self,mid_point):\n",
    "        # same thing but without check since 256 bp window is within the 1024bp window\n",
    "        mid_point = list(range(mid_point-128, mid_point + 128))\n",
    "        return mid_point\n",
    "    \n",
    "    \n",
    "    \"\"\"Function: Takes Total Tn5 counts per chromosome and counts per window\n",
    "          Calculates the Atac normalization of Tn5 cut sites counts\n",
    "          Tn5_counts_window = Tn5 bp window that corresponds to the OHE region of the genome (X feature)\n",
    "          C = sum of the Counts per window(summed bp region)\n",
    "          T = total_tn5_counts_per_chrom for h5py file\n",
    "    \"\"\"\n",
    "    def normalize_atac_reads(self, Tn5_counts_window, T,d):\n",
    "        self.Tn5_counts_window = Tn5_counts_window\n",
    "        self.T = T\n",
    "        counts = []\n",
    "        \n",
    "        for i in range(len(Tn5_counts_window)):\n",
    "            C = np.sum(Tn5_counts_window[i])\n",
    "            self.d['C'].append(C) \n",
    "            length = len(Tn5_counts_window[i])\n",
    "            norm = (C*10**9/(T*(length)))\n",
    "            counts.append(norm)\n",
    "        return counts\n",
    "    \n",
    "    \"\"\"Function: Window size and bins by bin_size\n",
    "          window = 256 bp window\n",
    "          bin_size = 1,2,4,6,8,16,32,64,128,256\n",
    "    \"\"\"\n",
    "    def bin_window(self, window,  bin_size):\n",
    "        self.window = window\n",
    "        self.bin_size = bin_size \n",
    "        length_window = len(window)\n",
    "        bins = []\n",
    "        for i in range(0, length_window, bin_size):\n",
    "            bins.append((window[i:i+bin_size]))\n",
    "        return bins\n",
    "    \"\"\"Function: returns length of dataset of X*Y\n",
    "    \"\"\"\n",
    "    def __len__(self):\n",
    "        return (len(self.unified_bed_file_df)*len(self.frag_tn5_h5_df_1))\n",
    "    \n",
    "    \"\"\"Function: returns X,Y from OHE genome h5py file and Tn5 frag counts h5py file\n",
    "    \n",
    "    \"\"\"\n",
    "    def __getitem__(self,index: int):\n",
    "        #Clears C list every iteration\n",
    "        self.d['C'].clear()\n",
    "        self.d['y'].clear()\n",
    "        #create sequence of strings for keys\n",
    "        dict_keys = ('Chrom','1024_bp_start_end','Patient_ID','y','C')\n",
    "        #create the dictionary, `my_dictionary`, using the fromkeys() method\n",
    "        self.d.fromkeys(dict_keys)\n",
    "        #print('index',index)\n",
    "        bam_file_index = (index % len(self.frag_tn5_h5_df_1))\n",
    "        peak_index = (index // len(self.frag_tn5_h5_df_1))\n",
    "        # object that contains chrom, start, end\n",
    "        coords = self.unified_bed_file_df.iloc[peak_index]\n",
    "        # Picks midpoint from coors(bed file peak region)\n",
    "        \n",
    "        # Add chrom coords to dictionary \n",
    "        self.d['Chrom'] = coords.chrom\n",
    "        \n",
    "        window = (self.get_mid_point(coords.start,coords.end))\n",
    "        # create X from OHE genome at the bed file peak region with 1024 bp window size\n",
    "        # Window for x and dictionary\n",
    "        temp = self.get_x_window(window)\n",
    "        x =  self.one_hot_enc_genome[coords.chrom]['onehot'][temp]\n",
    "        # Add start,end window coords to dictionary\n",
    "        #self.d['1024_bp_start_end'] = temp[::len(temp)-1]\n",
    "        \n",
    "\n",
    "        # To tensor when running on runAI\n",
    "        x = torch.as_tensor(x,  dtype=self.dtype)\n",
    "        # reads in h5py path from Col 1(treatment type) in df of h5py Tn5 insertion cutsite h5 paths\n",
    "        self.get_h5(self.frag_tn5_h5_df_1.iloc[ bam_file_index, self.frag_tn5_h5_df_1.columns.get_loc('diff_frag_h5_path')])\n",
    "        \n",
    "        # Add patient id to dictionary\n",
    "        self.d['Patient_ID'] = (self.frag_tn5_h5_df_1.iloc[ bam_file_index, self.frag_tn5_h5_df_1.columns.get_loc('patient_id')])\n",
    "        #Gets the h5py path at the bam index from dictionary of h5py paths\n",
    "        h5_tn5_read_count_diff = self.h5_files.get(self.frag_tn5_h5_df_1.iloc[ bam_file_index, self.frag_tn5_h5_df_1.columns.get_loc('diff_frag_h5_path')])\n",
    "        \n",
    "        \n",
    "        # Total total_tn5_counts for all chroms\n",
    "        T_1 = h5_tn5_read_count_diff.attrs[\"total_sum\"]\n",
    "        #Adds T_1 Tn5 counts value to dictionary\n",
    "        #self.d['T1_diff_counts'] = T_1\n",
    "        \n",
    "        # Sets y1 to 256 bp window from h5py Tn5 insertion cutsites corresponding to OHE genome window\n",
    "        y_1  = (h5_tn5_read_count_diff[coords.chrom]['depth'][self.get_y_window(window)])\n",
    "        # Bin window based off bin_size\n",
    "        y_1 = self.bin_window(window=y_1,bin_size=self.bin_size)\n",
    "        #Normalize each bin windowy_1 = self.bin_window(window=y_1,bin_size=self.bin_size)\n",
    "        norm_1 = self.normalize_atac_reads(y_1, T_1, self.d)\n",
    "        #self.d['Norm_1'] = norm_1\n",
    "\n",
    "        # reads in h5py path from Col 2(treatment type) in df of h5py Tn5 insertion cutsite h5 paths\n",
    "        self.get_h5(self.frag_tn5_h5_df_2.iloc[ bam_file_index, self.frag_tn5_h5_df_2.columns.get_loc('int_gamma_diff_frag_h5_path')])\n",
    "        \n",
    "        #Gets the h5py path at the bam index from dictionary of h5py paths\n",
    "        h5_tn5_read_count_int_gamma = self.h5_files.get(self.frag_tn5_h5_df_2.iloc[ bam_file_index, self.frag_tn5_h5_df_2.columns.get_loc('int_gamma_diff_frag_h5_path')])\n",
    "        # Total total_tn5_counts for all chroms\n",
    "        T_2 = h5_tn5_read_count_int_gamma.attrs[\"total_sum\"]\n",
    "        #Adds T_2 Tn5 counts value to dictionary\n",
    "        #self.d['T2_int_gamma_diff_counts'] = T_2\n",
    "        # Sets y2 to 256 bp window from h5py Tn5 insertion cutsites corresponding to OHE genome window\n",
    "        y_2 = (h5_tn5_read_count_int_gamma[coords.chrom]['depth'][self.get_y_window(window)])\n",
    "        # Bin window based off bin_size\n",
    "        y_2 = self.bin_window(window=y_2,bin_size=self.bin_size)\n",
    "        #Normalize bins in window\n",
    "        norm_2 = self.normalize_atac_reads(y_2, T_2, self.d)\n",
    "        #self.d['Norm_2'] = norm_2\n",
    "        # reads in h5py path from Col 3(treatment type) in df of h5py Tn5 insertion cutsite h5 paths\n",
    "        self.get_h5(self.frag_tn5_h5_df_3.iloc[ bam_file_index, self.frag_tn5_h5_df_3.columns.get_loc('salm_int_gamma_diff_frag_h5_path')])\n",
    "        #Gets the h5py path at the bam index from dictionary of h5py paths\n",
    "        h5_tn5_read_count_salm_int_gamma = self.h5_files.get(self.frag_tn5_h5_df_3.iloc[ bam_file_index, self.frag_tn5_h5_df_3.columns.get_loc('salm_int_gamma_diff_frag_h5_path')])\n",
    "        # Total total_tn5_counts for all chroms\n",
    "        T_3 = h5_tn5_read_count_salm_int_gamma.attrs[\"total_sum\"]\n",
    "        #Adds T_3 Tn5 counts value to dictionary\n",
    "        #self.d['T3_salm_int_gamma_counts'] = T_3\n",
    "        \n",
    "        \n",
    "        # Sets y3 to 256 bp window from h5py Tn5 insertion cutsites corresponding to OHE genome window\n",
    "        y_3 = (h5_tn5_read_count_salm_int_gamma[coords.chrom]['depth'][self.get_y_window(window)])\n",
    "        # Bin window based off bin_size\n",
    "        y_3 = self.bin_window(window=y_3,bin_size=self.bin_size)\n",
    "        #Normalize bins in window\n",
    "        norm_3 = self.normalize_atac_reads(y_3, T_3, self.d)\n",
    "        #self.d['Norm_3'] = norm_3\n",
    "\n",
    "        # reads in h5py path from Col 4(treatment type) in df of h5py Tn5 insertion cutsite h5 paths\n",
    "        self.get_h5(self.frag_tn5_h5_df_4.iloc[ bam_file_index, self.frag_tn5_h5_df_4.columns.get_loc('salm_diff_frag_h5_path')])\n",
    "        #Gets the h5py path at the bam index from dictionary of h5py paths\n",
    "        h5_tn5_read_count_salm_diff = self.h5_files.get(self.frag_tn5_h5_df_4.iloc[ bam_file_index, self.frag_tn5_h5_df_4.columns.get_loc('salm_diff_frag_h5_path')])\n",
    "        T_4 = h5_tn5_read_count_salm_diff.attrs[\"total_sum\"]\n",
    "        \n",
    "        #Adds T_4 Tn5 counts value to dictionary\n",
    "        #self.d['T4_salm_counts'] = T_4\n",
    "        \n",
    "        # Sets y4 to 256 bp window from h5py Tn5 insertion cutsites corresponding to OHE genome window\n",
    "        y_4 = (h5_tn5_read_count_salm_diff[coords.chrom]['depth'][self.get_y_window(window)])\n",
    "        # Bin window based off bin_size\n",
    "        y_4 = self.bin_window(window=y_4,bin_size=self.bin_size)\n",
    "        #Normalize bins in window\n",
    "        norm_4 = self.normalize_atac_reads(y_4, T_4, self.d)\n",
    "        #self.d['Norm_4'] = norm_4\n",
    "        # concatenate 4 windows(based off bin_size) to create concatenated bp window of four treatments corresponding to OHE genome 1024 bp window\n",
    "        y = np.concatenate((norm_1,norm_2,norm_3,norm_4))\n",
    "        self.d['y'].append(y) \n",
    "        # To tensor when running on runAI\n",
    "        y = torch.as_tensor(y, dtype=self.dtype)\n",
    "\n",
    "        #Returns tuple of X,y ((1024,4),(bin size related)) size\n",
    "        return x, y, self.d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.]]),\n",
       " tensor([0.9952, 0.4280, 0.1627, 0.2674]),\n",
       " defaultdict(list,\n",
       "             {'C': [12, 6, 2, 4],\n",
       "              'y': [array([0.99522483, 0.42800735, 0.16273491, 0.26738884])],\n",
       "              'Chrom': 'chr1',\n",
       "              '1024_bp_start_end': (180367, 181391),\n",
       "              'Patient_ID': 'HPSI0114i-oevr_3'}))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test index of no N's index\n",
    "\n",
    "DS = Dataset_class(one_hot_enc_genome, train_bed_file_df, frag_tn5_train_df,bin_size = 256)\n",
    "#type(DS[0])\n",
    "#len(DS)\n",
    "DS.__getitem__(0)\n",
    "#DS.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#np.random.choice(5, 3, replace=False)\n",
    "# DS.__len__()\n",
    "#indices = np.arange(0,2)\n",
    "\n",
    "indices = np.random.choice(DS.__len__(), 10, replace=False)\n",
    "#np.random.choice(5, 3, )\n",
    "subset_10 = data_utils.Subset(DS, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.Subset"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(subset_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n",
      "7\n",
      "7\n",
      "2\n",
      "<class 'list'>\n",
      "2\n",
      "28\n",
      "13\n",
      "10\n",
      "<class 'list'>\n",
      "22\n",
      "21\n",
      "26\n",
      "41\n",
      "<class 'list'>\n",
      "3\n",
      "2\n",
      "4\n",
      "2\n",
      "<class 'list'>\n",
      "70\n",
      "33\n",
      "37\n",
      "102\n",
      "<class 'list'>\n",
      "4\n",
      "1\n",
      "7\n",
      "7\n",
      "<class 'list'>\n",
      "1\n",
      "1\n",
      "70\n",
      "40\n",
      "<class 'list'>\n",
      "38\n",
      "7\n",
      "19\n",
      "58\n",
      "<class 'list'>\n",
      "1\n",
      "5\n",
      "3\n",
      "15\n",
      "<class 'list'>\n",
      "137\n",
      "233\n",
      "86\n",
      "380\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in (subset_10):\n",
    "    #print(i)\n",
    "    print(type(i[2]['C']))\n",
    "    for k in i[2]['C']:\n",
    "            print(k)\n",
    "    #df = pd.DataFrame([[k] + v[0] for k, v in i[2].items()], \n",
    "                   #columns=['id', 'score', 'category'])\n",
    "        #print((data_list))\n",
    "        #print('yo')\n",
    "        #data = pd.Series(data_list)\n",
    "        #print(data)\n",
    "        #dict_new = dict(data_list)\n",
    "        #df = pd.concat(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = []\n",
    "for i in subset_10:\n",
    "    #print(i)\n",
    "    for data_list in i[2]['C']:\n",
    "        C_list.append(data_list)  # Unpack\n",
    "        #print(x) # Not handled in example\n",
    "    plt.xlabel('C = count values summed during normalization for 256 bin size')\n",
    "    # naming the y axis\n",
    "    plt.ylabel('10,000 Random indexes out of total 7,213,673 indexes from Training dataset')\n",
    "    # giving a title to my graph\n",
    "    plt.title('10,000 random data points for C from Training dataset')\n",
    "    plt.hist((C_list),width=20, color='g')\n",
    "    # naming the x axis\n",
    "    C_list.clear()\n",
    "        #plt.plot(dates, values, label=key)\n",
    "    #plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_list = []\n",
    "\n",
    "for i in subset_10:\n",
    "    #print(i)\n",
    "    for data_list in i[2]:\n",
    "        #print(data_list)\n",
    "        y_list.append(data_list)  # Unpack\n",
    "        #print(x) # Not handled in example\n",
    "    plt.xlabel('Normalized count values summed during normalization for 256 bin size')\n",
    "    # naming the y axis\n",
    "    plt.ylabel('10,000 Random indexes out of total 7,213,673 indexes from Training dataset')\n",
    "    # giving a title to my graph\n",
    "    plt.title('10,000 random data points for Normalized counts from Training dataset')\n",
    "    plt.hist((y_list),width=5, color='g')\n",
    "    # naming the x axis\n",
    "    y_list.clear()\n",
    "        #plt.plot(dates, values, label=key)\n",
    "    #plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for graphing X(OHE Genome for 1024 bp window) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import modisco\n",
    "import sys\n",
    "import os\n",
    "import modisco.visualization\n",
    "from modisco.visualization import viz_sequence\n",
    "\n",
    "\n",
    "viz_sequence.plot_weights(DS[6][0], subticks_frequency=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for graphing Y(Tn5 insertion cut counts in 256 bp window) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchplot as plt\n",
    "\n",
    "# Takes in y tensor index from dataset class tuple in DS[#][1] format. First index value is the total index for the tuple. Second index is 0 for X and 1 for Y for 2nd parameter. \n",
    "def graph_y(y_tensor_index):\n",
    "    #Sets x axis variable for graph\n",
    "    x = len(y_tensor_index)\n",
    "    # Sets Y axis variable for graph\n",
    "    y = y_tensor_index\n",
    "    # Plots with torch.arrange since we have a tensor object.\n",
    "    plt.bar(torch.arange(x),y, width = 1)\n",
    "    plt.xlabel(f\"{x} BP window length, bin_size={DS.bin_size}\")\n",
    "    plt.ylabel(\"No.of Tn5 read insertion cut sites counts\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call graph function\n",
    "graph_y(y_tensor_index=DS[6][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports for pytorch lightning and wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "# Pytorch modules\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "import pytorch_lightning as pl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definining our model(Naive CC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitNN(LightningModule):\n",
    "    def __init__(self,seq_len=1024, channels= 4,num_classes=16, n_layer_1=128, n_layer_2=64, n_layer_3=32 ,lr=1e-3):\n",
    "        \"\"\"\n",
    "        init convolution and activation layers\n",
    "        Args:\n",
    "        x: (Nx1x2004)\n",
    "        class: \n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__() \n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(channels, n_layer_1, kernel_size=3)\n",
    "        self.batch1 = nn.BatchNorm1d(n_layer_1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = torch.nn.Conv1d(n_layer_1, n_layer_2, kernel_size=3)\n",
    "        self.batch2 = nn.BatchNorm1d(n_layer_2)\n",
    "        self.conv3 = torch.nn.Conv1d(n_layer_2, n_layer_3, kernel_size=3)\n",
    "        self.batch3 = nn.BatchNorm1d( n_layer_3)\n",
    "        self.pool = torch.nn.MaxPool1d(4)\n",
    "        self.fc1 = torch.nn.Linear(480, num_classes)\n",
    "        \n",
    "        # optimizer parameters\n",
    "        self.lr = lr\n",
    "\n",
    "        # metrics\n",
    "        self.R2score = torchmetrics.R2Score(num_outputs = 16)\n",
    "\n",
    "        # optional - save hyper-parameters to self.hparams\n",
    "        # they will also be automatically logged as config parameters in W&B\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward function describes how input tensor is transformed to output tensor\n",
    "        Args:\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size,  seq_len, channels = x.size()\n",
    "        # Changes the order to batch_size, channels, seq_len \n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #Flatten layer for fully connected layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''needs to return a loss from a single batch'''\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.mse_loss(logits, y)\n",
    "\n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('train_R2', self.R2score(logits, y))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.mse_loss(logits, y)\n",
    "        \n",
    "        # Log validation loss (will be automatically averaged over an epoch)\n",
    "        self.log('valid_loss', loss)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('valid_R2', self.R2score(logits,  y))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.mse_loss(logits, y)\n",
    "\n",
    "        # Log test loss\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('test_R2', self.R2score(logits, y))\n",
    "        return loss\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        pred = self(x)\n",
    "        return pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''defines model optimizer'''\n",
    "        return Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = LitNN()\n",
    "batch_size = 64\n",
    "summary(model, input_size=(batch_size, 1024,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old lightning Mnist code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitNN(LightningModule):\n",
    "\n",
    "    def __init__(self, seq_len=1024, seq_len_tn5=1024,channels= 4,num_classes=1024, n_layer_1=1280, n_layer_2=640, lr=1e-3):\n",
    "        '''method used to define our model parameters'''\n",
    "        super().__init__()\n",
    "    \n",
    "        # Dimension of X is (1024,4). Window of OHE genome bp x 5(OHE ACGTN)\n",
    "        self.layer_1 = torch.nn.Conv1d(seq_len * channels, n_layer_1)\n",
    "        self.layer_2 = torch.nn.Conv1d(n_layer_1, n_layer_2)\n",
    "        self.layer_3 = torch.nn.Conv1d(n_layer_2, num_classes)\n",
    "\n",
    "        # optimizer parameters\n",
    "        self.lr = lr\n",
    "\n",
    "        # metrics\n",
    "        self.R2score = torchmetrics.R2Score(num_outputs = 1024)\n",
    "\n",
    "        # optional - save hyper-parameters to self.hparams\n",
    "        # they will also be automatically logged as config parameters in W&B\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''method used for inference input -> output'''\n",
    "        #print('x dim pre view',x.size())\n",
    "        batch_size, seq_len, channels = x.size()\n",
    "        # (b, 1024, 4) -> (b, 1024*4)\n",
    "        #print('x dim post forward',x.size())\n",
    "        x = x.view(batch_size, -1)\n",
    "        #print('x dim post view',x.size())\n",
    "        x = self.layer_1(x)\n",
    "        #print('x dim post layer 1',x.size())\n",
    "\n",
    "        x = F.relu(x)\n",
    "        #print('x dim post relu 1',x.size())\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        #print('x dim post layer 2',x.size())\n",
    "        x = F.relu(x)\n",
    "        #print('x dim post relu 2',x.size())\n",
    "        x = self.layer_3(x)\n",
    "        #print('nn x post layer 3', x.size())\n",
    "        x = F.relu(x)\n",
    "        #print('returned nn x post relu 3', x.size())\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''needs to return a loss from a single batch'''\n",
    "        x, y = batch\n",
    "        #print('x dim in trstep',x.size())\n",
    "        #print('y dim in trstep',y.size())\n",
    "\n",
    "\n",
    "        logits = self(x)\n",
    "        loss = F.mse_loss(logits, y)\n",
    "\n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('train_R2', self.R2score(logits, y))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        x, y = batch\n",
    "        #print('x dim in vstep',x.size())\n",
    "        #print('y dim in vstep',y.size())\n",
    "\n",
    "        logits = self(x)\n",
    "        #print('logits',logits.size())\n",
    "        #print('y in vald step',y.size())\n",
    "        loss = F.mse_loss(logits, y)\n",
    "        #print('loss',loss)\n",
    "        # Log validation loss (will be automatically averaged over an epoch)\n",
    "        self.log('valid_loss', loss)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('valid_R2', self.R2score(logits,  y))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        x, y = batch\n",
    "        #print('x dim in tstep',x.size())\n",
    "        #print('y dim in tstep',y.size())\n",
    "\n",
    "        logits = self(x)\n",
    "        loss = F.mse_loss(logits, y)\n",
    "\n",
    "        # Log test loss\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('test_R2', self.R2score(logits, y))\n",
    "        return loss\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        pred = self(x)\n",
    "        return pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''defines model optimizer'''\n",
    "        return Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Loader functions(train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define collate function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import torch\n",
    "\n",
    "class ATACDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, one_hot_enc_genome, train_bed_file_df,valid_bed_file_df,test_bed_file_df,frag_tn5_train_df,frag_tn5_valid_df,frag_tn5_test_df, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.one_hot_enc_genome = one_hot_enc_genome\n",
    "        self.train_bed_file_df = train_bed_file_df\n",
    "        self.valid_bed_file_df = valid_bed_file_df\n",
    "        self.test_bed_file_df = test_bed_file_df\n",
    "        self.frag_tn5_train_df = frag_tn5_train_df\n",
    "        self.frag_tn5_valid_df = frag_tn5_valid_df\n",
    "        self.frag_tn5_test_df = frag_tn5_test_df\n",
    "        self.batch_size = batch_size\n",
    "    # collate function for removing none from batch\n",
    "    # def collate_fn(self, batch):\n",
    "    #     batch = list(filter(lambda x: x is not None, batch))\n",
    "    #     return torch.utils.data.dataloader.default_collate(batch)\n",
    "    def setup(self, stage=None):\n",
    "        '''called on each GPU separately - stage defines if we are at fit or test step'''\n",
    "        # we set up only relevant datasets when stage is specified (automatically set by Pytorch-Lightning)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_ds = Dataset_class(one_hot_enc_genome, train_bed_file_df, frag_tn5_train_df, dtype = torch.float32, bin_size = 64)\n",
    "            self.valid_ds = Dataset_class(one_hot_enc_genome, valid_bed_file_df, frag_tn5_valid_df,dtype = torch.float32, bin_size = 64)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_ds = Dataset_class(one_hot_enc_genome, test_bed_file_df, frag_tn5_test_df,bin_size = 64)\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        '''returns training dataloader'''\n",
    "        train_dl = DataLoader(self.train_ds, shuffle=True, batch_size=self.batch_size, num_workers=16)\n",
    "        return train_dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        '''returns validation dataloader'''\n",
    "        valid_dl = DataLoader(self.valid_ds,shuffle=False, batch_size=self.batch_size,num_workers=16)\n",
    "        return valid_dl\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        '''returns test dataloader'''\n",
    "        test_dl = DataLoader(self.test_ds,  shuffle=False,  batch_size=self.batch_size,num_workers=16)\n",
    "        # collate_fn=self.collate_fn\n",
    "        # , num_workers=4\n",
    "        return test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "# Setup  model\n",
    "model = LitNN(seq_len=1024, channels= 4,num_classes=16, n_layer_1=128, n_layer_2=64, n_layer_3=32,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ATAC Data module\n",
    "atac = ATACDataModule(one_hot_enc_genome, train_bed_file_df,valid_bed_file_df,test_bed_file_df,frag_tn5_train_df,frag_tn5_valid_df,frag_tn5_test_df, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer initializer\n",
    "trainer = pl.Trainer(\n",
    "    gradient_clip_val=100000,\n",
    "    logger=wandb_logger,\n",
    "    #overfit_batches= 0.1,\n",
    "    # fast_dev_run = True,\n",
    "    accelerator='auto',\n",
    "    auto_lr_find=True, # W&B integration \n",
    "    #devices=1 if torch.cuda.is_available() else None,\n",
    "    max_epochs=3 # number of epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best LR automatically\n",
    "trainer.tune(model,atac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder = trainer.tuner.lr_find(model, atac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LR with the best chosen LR\n",
    "model.lr = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(model, atac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "trainer.test(model, atac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clos wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for testing H5Py files depth whether read counts existed(QC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for i in range(len(hd5_tn_test['chr1']['depth'])):\n",
    "    if hd5_tn_test['chr1']['depth'][i] > 0:\n",
    "        counter +=1\n",
    "\n",
    "\n",
    "   # hd5_tn_test['chr1']['depth'][()]>0\n",
    "\n",
    "print('The length of data with a for loop: {}'.format((counter)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0,'read_depth'] = Path(df.loc[0]['bam paths']).parent / \"read_depths.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for i in ((ds)):\n",
    "        if ds[i] != '[0 0 0 0]':\n",
    "            counter += 0 \n",
    "\n",
    "   # hd5_tn_test['chr1']['depth'][()]>0\n",
    "\n",
    "print('The length of data with a for loop: {}'.format((counter)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check h5 file structure(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd5_tn_test = h5py.File('/home/jovyan/data3/jjaureguy/PRJEB18997/10_genos/10_genos_fastq/HPSI0114i-eipl_1/out/treatments/diff_mb/frag_depths.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd5_tn_test\n",
    "\n",
    "#hd5_tn_test['chrome']['depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hd5_tn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(one_hot_enc_genome.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enc_genome.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = one_hot_enc_genome['chr1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_frame_env]",
   "language": "python",
   "name": "conda-env-data_frame_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "70048b412bdf50bb9639879095d5b7e9588630cc3326e9b869d915719d9eeab2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
