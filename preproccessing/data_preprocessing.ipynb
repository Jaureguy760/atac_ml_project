{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Dict approach\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "#from typing import Any, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import torch\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "#import pytorch_lightning as pl\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load genome loader for salk cluster\n",
    "sys.path.append('/iblm/netapp/home/jjaureguy/genome_loader/genome-loader')\n",
    "\n",
    "sample_path = os.listdir('/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/10_genos_fastq')\n",
    "genos_path = os.listdir('/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/19_genos_fastq')\n",
    "total_path = sample_path + genos_path\n",
    "test_fasta= '/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/hg38/ref_genome/hg38.fa'\n",
    "\n",
    "\n",
    "out_dir = '/iblm/netapp/data4/jjaureguy/out_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load genome loader for runai-GPU cluster\n",
    "sys.path.append('/home/jovyan/home/jjaureguy/genome_loader/genome-loader')\n",
    "\n",
    "sample_path = os.listdir('/home/jovyan/data3/jjaureguy/PRJEB18997/10_genos/10_genos_fastq')\n",
    "genos_path = os.listdir('/home/jovyan/data3/jjaureguy/PRJEB18997/10_genos/19_genos_fastq')\n",
    "total_path = sample_path + genos_path\n",
    "test_fasta= '/home/jovyan/data3/jjaureguy/PRJEB18997/10_genos/hg38/ref_genome/hg38.fa'\n",
    "\n",
    "# change to /iblm/netapp/data4/jjaureguy/ref_genome/WholeGenomeFasta/genome.fa\n",
    "\n",
    "\n",
    "\n",
    "out_dir = '/home/jovyan/data4/jjaureguy/out_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append('/home/jovyan/w')\n",
    "import genome_loader.write_h5\n",
    "import genome_loader.encode_data\n",
    "import genome_loader.get_data\n",
    "import genome_loader.get_encoded\n",
    "import genome_loader.load_data\n",
    "import genome_loader.load_h5\n",
    "from genome_loader.write_h5 import write_encoded_genome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Pandas DF Text VCF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('/iblm/netapp/data3/jjaureguy/PRJEB18997/genotypes/ftp_imputed.txt',sep='\\t')\n",
    "df_2 = pd.read_csv('/iblm/netapp/data3/jjaureguy/PRJEB18997/genotypes/ftp_ped.txt',sep='\\t')\n",
    "df_3 = pd.read_csv('/iblm/netapp/data3/jjaureguy/PRJEB18997/genotypes/ftp_tbi.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_1 = df_1[df_1['imputed_phased genotypes.vcf.gz'].str.endswith('.gz')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_1.rename(columns = {'imputed_phased genotypes.vcf.gz':'vcf'}, inplace = True)\n",
    "df_1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2[df_2['ped'].str.endswith('.gz')]\n",
    "df_2.rename(columns = {'ped':'vcf'}, inplace = True)\n",
    "df_2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_3[df_3['imputed_phased genotypes.vcf.gz.tbi'].str.endswith('.gz')]\n",
    "df_3.rename(columns = {'imputed_phased genotypes.vcf.gz.tbi':'vcf'}, inplace = True)\n",
    "df_3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_1, df_2, df_3]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('/iblm/netapp/data3/jjaureguy/PRJEB18997/phased_vcf/vcf.txt', sep='\\t', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write ohe ref genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_encoded_genome(test_fasta, out_dir, h5_name='genome_onehot.h5', encode_spec= \"ACGT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dir = \"/iblm/netapp/data4/jjaureguy/out_dir\"\n",
    "treatments_path = os.listdir('/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/10_genos_fastq/HPSI0114i-eipl_1/out/treatments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bedfile DF of first 5(10 ids) interesected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unified_bed_df = pd.read_csv('/home/jovyan/data3/jjaureguy/PRJEB18997/10_genos/text_files/unified_peak_set/unified_peak.txt',sep='\\t', header=None, names=['chrom', 'start', 'end'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check chromosome naming in list\n",
    "unique = unified_bed_df['chrom'].unique()\n",
    "# generate a list of chromosomes 1-22 without X and Y and mitochondrial \n",
    "chr_list =  [ 'chr{}'.format(x) for x in list(range(1,23)) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter chromosomes to chrom 1-22 without X and Y(only looking at autosomes)\n",
    "unified_bed_df = unified_bed_df[unified_bed_df['chrom'].isin(chr_list)]\n",
    "unified_bed_df['chrom'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_bed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index after filtering\n",
    "unified_bed_df = unified_bed_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_bed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove chromosome 3 and 2 out of chrom list to be used downstream for generating train/valid/test dataframes\n",
    "chr_list.remove('chr3')\n",
    "chr_list.remove('chr2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure we have correct list for training data frame\n",
    "chr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train/test/valid \n",
    "train_chrom = chr_list\n",
    "valid_chrom = ['chr2']\n",
    "test_chrom = ['chr3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter DF by boolean values for train/valid/test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unified_bed_df[\"train\"] = (unified_bed_df['chrom'].isin(train_chrom))\n",
    "unified_bed_df[\"valid\"] = (unified_bed_df['chrom'].isin(valid_chrom ))\n",
    "unified_bed_df[\"test\"] = (unified_bed_df['chrom'].isin(test_chrom))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_bed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure boolean values are correct\n",
    "unified_bed_df['valid'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure boolean values are correct\n",
    "unified_bed_df['test'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure boolean values are correct\n",
    "\n",
    "unified_bed_df['train'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set value variable to true\n",
    "value = True\n",
    "# Filter through boolean values and generate dataframes for train/test/valid\n",
    "train_df = unified_bed_df.loc[unified_bed_df['train'] == True, ['chrom', 'start', 'end']]\n",
    "valid_df = unified_bed_df.loc[unified_bed_df['valid'] == True, ['chrom', 'start', 'end']]\n",
    "test_df = unified_bed_df.loc[unified_bed_df['test'] == True, ['chrom', 'start', 'end']]\n",
    "\n",
    "#Generate csv text files of dataframes for dataset class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = valid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate CSV files of train/test/split dataframes for salk cluster\n",
    "train_df.to_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/train_df.txt', sep='\\t', index=False, header=True)\n",
    "test_df.to_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/test_df.txt', sep='\\t', index=False, header=True)\n",
    "valid_df.to_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/valid_df.txt', sep='\\t', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate CSV files of train/test/split dataframes for runAI\n",
    "train_df.to_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/train_df_runai.txt', sep='\\t', index=False, header=True)\n",
    "test_df.to_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/test_df_runai.txt', sep='\\t', index=False, header=True)\n",
    "valid_df.to_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/valid_df_runai.txt', sep='\\t', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_bed_df.to_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/unified_bed_df.txt', sep='\\t', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load back in already finished above.\n",
    "\n",
    "unified_bed_df = pd.read_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/unified_bed_df.txt',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_bed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_bed_df['chrom'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe for patient ID, treatment type, H5 Read depth(atac output for tn5 fragment insertation cut sites), replace old files once completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"patient_id\",\"diff_frag_h5_path\",\"int_gamma_diff_frag_h5_path\",\"salm_int_gamma_diff_frag_h5_path\",\"salm_diff_frag_h5_path\"]\n",
    "\n",
    "df_read_depth_h5 = pd.DataFrame(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "bam_paths = []\n",
    "rest_bam_paths = []\n",
    "peak_paths = []\n",
    "treatment = []\n",
    "\n",
    "for i in (total_path):\n",
    "    #runai\n",
    "    path = f'/home/jovyan/data3/jjaureguy/PRJEB18997/10_genos/10_genos_fastq/{i}/out/treatments/*/frag_depths.h5'\n",
    "    path_rest = f'/home/jovyan/data3/jjaureguy/PRJEB18997/10_genos/19_genos_fastq/{i}/out/treatments/*/frag_depths.h5'\n",
    "   \n",
    "    # #salk\n",
    "    # path = f'/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/10_genos_fastq/{i}/out/treatments/*/frag_depths.h5'\n",
    "    # path_rest = f'/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/19_genos_fastq/{i}/out/treatments/*/frag_depths.h5'\n",
    "    for filename in glob.glob(path):\n",
    "        bam_paths.append(filename)\n",
    "    for filename in glob.glob(path_rest):\n",
    "        rest_bam_paths.append(filename)\n",
    "\n",
    "read_depth_frag_h5_paths = bam_paths + rest_bam_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_h5_lens(in_h5, dataset_id=None):\n",
    "    \n",
    "    with h5py.File(in_h5, \"a\") as file:\n",
    "        \n",
    "        if not dataset_id:\n",
    "            dataset_id = file.attrs[\"id\"]\n",
    "\n",
    "        for chrom in file.keys():\n",
    "            chrom_data = file[chrom][dataset_id]\n",
    "            file[chrom].attrs[\"length\"] = chrom_data.shape[0]\n",
    "            print(f\"{chrom}: {file[chrom].attrs['length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in read_depth_frag_h5_paths:\n",
    "    print('started writing h5 lens',{i})\n",
    "    write_h5_lens(i)\n",
    "    print('finished writing h5 lens',{i})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "id_re= re.compile(\".*HPSI\")\n",
    "treatment_re = re.compile(\".*_mb\")\n",
    "id_list = []\n",
    "treatment_list = []\n",
    "for i in read_depth_frag_h5_paths:\n",
    "    p = Path(i)\n",
    "    #print(p)\n",
    "    split = (p.parts)\n",
    "    id = list(filter(id_re.match, split))\n",
    "    treatment = list(filter(treatment_re.match, split))\n",
    "    id_list.append(id)\n",
    "    treatment_list.append(id_list)\n",
    "    \n",
    "\n",
    "id_list = np.unique(id_list)\n",
    "#df_read_depth_h5['patient_id'] = np.unique(id_list)\n",
    "#df_read_depth_h5['treatment'] = treatment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of patient ids only containing 2 treatments\n",
    "imbalanced_treatments = ['HPSI0913i-eika_2', 'HPSI0114i-iisa_3','HPSI1213i-nusw_2', 'HPSI1213i-pahc_4', 'HPSI0114i-joxm_1', 'HPSI1113i-ieki_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " id_list = [x for x in id_list if x not in imbalanced_treatments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bam_paths = bam_paths + rest_bam_paths \n",
    "len(total_bam_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_list = [s for s in total_bam_paths if str(id_list) in s]\n",
    "\n",
    "#new_list = [s for s in total_bam_paths if id_list in s]\n",
    "# Using list comprehension\n",
    "output = [b for b in total_bam_paths if\n",
    "          all(a not in b for a in id_list)]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bam_paths = [x for x in total_bam_paths if x not in output]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_bam_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bam_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the h5 file paths list\n",
    "print(len(total_bam_paths))\n",
    "\n",
    "# strings to be checked by file paths\n",
    "diff = 'diff_mb'\n",
    "int_gamma_diff = 'int_gamma_mb'\n",
    "salm_int_gamma = 'salm_int_gamma_mb'\n",
    "salm_diff = 'salm_mb'\n",
    "\n",
    "# create sublist for colum and to be removed from total bam paths since total bam paths has 2 simiar regex pattern conflict\n",
    "salm_int_gamma = [s for s in total_bam_paths if salm_int_gamma in s]\n",
    "print('len of sam int gamma', len(salm_int_gamma))\n",
    "# Create \n",
    "total_bam_paths = [x for x in total_bam_paths if x not in salm_int_gamma]\n",
    "\n",
    "print('total bp',len(total_bam_paths))\n",
    "\n",
    "\n",
    "#print((total_bam_paths))\n",
    "#print(type(diff_mb))\n",
    "# List comprehension to add paths to approriate treatment columns\n",
    "diff = [s for s in total_bam_paths if diff in s]\n",
    "int_gamma_diff = [s for s in total_bam_paths if int_gamma_diff in s]\n",
    "salm_diff = [s for s in total_bam_paths if salm_diff in s]\n",
    "\n",
    "\n",
    "# Set columns to new filtered lists of treatment paths\n",
    "df_read_depth_h5['diff_frag_h5_path'] = diff\n",
    "df_read_depth_h5['int_gamma_diff_frag_h5_path'] = int_gamma_diff\n",
    "df_read_depth_h5['salm_int_gamma_diff_frag_h5_path'] = salm_int_gamma\n",
    "df_read_depth_h5['salm_diff_frag_h5_path'] = salm_diff\n",
    "#df_read_depth_h5['read_depth_frag_h5_path'] = total_bam_paths\n",
    "#df_read_depth_h5['treatment'] = df_read_depth_h5['read_depth_frag_h5_path'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "id_re= re.compile(\".*HPSI\")\n",
    "treatment_re = re.compile(\".*_mb\")\n",
    "id_list = []\n",
    "print(id_list)\n",
    "treatment_list = []\n",
    "for i in df_read_depth_h5['diff_frag_h5_path']:\n",
    "    p = Path(i)\n",
    "    #print(p)\n",
    "    split = (p.parts)\n",
    "    id = list(filter(id_re.match, split))\n",
    "    treatment = list(filter(treatment_re.match, split))\n",
    "    id_list.append(id)\n",
    "    treatment_list.append(id_list)\n",
    "    \n",
    "\n",
    "#id_list = np.unique(id_list)\n",
    "df_read_depth_h5['patient_id'] = (id_list)\n",
    "#df_read_depth_h5['treatment'] = treatment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_read_depth_h5['patient_id'] = df_read_depth_h5['patient_id'].str[0]\n",
    "#df_read_depth_h5['treatment'] = df_read_depth_h5['treatment'].str[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5_final = df_read_depth_h5.loc[df_read_depth_h5['imbalanced_treatments'] == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_read_depth_h5['patient_id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_patient_list = df_read_depth_h5['patient_id'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_patient_list = np.array(id_patient_list.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(id_patient_list)\n",
    "#23\n",
    "#3\n",
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_patient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(0)\n",
    "train_list = np.random.choice(id_patient_list, 19, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remainder_list = set(id_patient_list)-set(train_list)\n",
    "remainder_list = list(remainder_list)\n",
    "remainder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "valid_list = np.random.choice(remainder_list, 2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = set(remainder_list)-set(valid_list)\n",
    "test_list = list(test_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5[\"train\"] = (df_read_depth_h5['patient_id'].isin(train_list))\n",
    "df_read_depth_h5[\"valid\"] = (df_read_depth_h5['patient_id'].isin(valid_list))\n",
    "df_read_depth_h5[\"test\"] = (df_read_depth_h5['patient_id'].isin(test_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5['train'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5['valid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5['test'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_depth_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set value variable to true\n",
    "value = True\n",
    "# Filter through boolean values and generate dataframes for train/test/valid\n",
    "train_df_tn5 = df_read_depth_h5.loc[df_read_depth_h5['train'] == True, ['patient_id', 'diff_frag_h5_path', 'int_gamma_diff_frag_h5_path','salm_int_gamma_diff_frag_h5_path', 'salm_diff_frag_h5_path']]\n",
    "valid_df_tn5 = df_read_depth_h5.loc[df_read_depth_h5['valid'] == True, ['patient_id', 'diff_frag_h5_path', 'int_gamma_diff_frag_h5_path', 'salm_int_gamma_diff_frag_h5_path', 'salm_diff_frag_h5_path']]\n",
    "test_df_tn5 = df_read_depth_h5.loc[df_read_depth_h5['test'] == True, ['patient_id', 'diff_frag_h5_path', 'int_gamma_diff_frag_h5_path', 'salm_int_gamma_diff_frag_h5_path', 'salm_diff_frag_h5_path']]\n",
    "\n",
    "#Generate csv text files of dataframes for dataset class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df_tn5 = train_df_tn5.reset_index(drop=True)\n",
    "valid_df_tn5 = valid_df_tn5.reset_index(drop=True)\n",
    "test_df_tn5 = test_df_tn5.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_tn5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_tn5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df_tn5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write CSVs for Train/valid/test and larger total df for Tn5 insertion cound dataframes for salk cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_read_depth_h5.to_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/frag_tn5_h5_df.txt', sep='\\t', index=False)\n",
    "train_df_tn5.to_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/salk_df/train_frag_tn5_h5_df.txt', sep='\\t', index=False)\n",
    "valid_df_tn5.to_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/salk_df/valid_frag_tn5_h5_df.txt', sep='\\t', index=False)\n",
    "test_df_tn5.to_csv('/iblm/netapp/data4/jjaureguy/jupyter/data_frames/salk_df/test_frag_tn5_h5_df.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write CSVs for Train/valid/test and larger total df for Tn5 insertion cound dataframes for runAI cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_tn5.to_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/runai_df/train_frag_tn5_h5_df_runai.txt', sep='\\t', index=False)\n",
    "valid_df_tn5.to_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/runai_df/valid_frag_tn5_h5_df_runai.txt', sep='\\t', index=False)\n",
    "test_df_tn5.to_csv('/home/jovyan/data4/jjaureguy/jupyter/data_frames/runai_df/test_frag_tn5_h5_df_runai.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary of bed and bam paths for 10 genos ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "bam_paths = []\n",
    "peak_paths = []\n",
    "treatment_paths = []\n",
    "\n",
    "\n",
    "\n",
    "column_names = [\n",
    "    #'patient id', \n",
    "                'treatments','bam paths', 'peak paths']\n",
    "\n",
    "d = {}\n",
    "\n",
    "#filenames = [f for f in filenames if (f.startswith(\"ERR\") and f.lower().endswith(\"1\"))]\n",
    "for i in (sample_path):\n",
    "    #print([i][0]\n",
    "    path = f\"/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/10_genos_fastq/{i}/out/treatments/*/merged_qc.bam\"\n",
    "    peak_path = f\"/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/10_genos_fastq/{i}/out/treatments/merged.bed\"\n",
    "    for filename in glob.glob(path):\n",
    "        bam_paths.append(filename)\n",
    "    for filename in glob.glob(peak_path):\n",
    "        peak_paths.append(filename)\n",
    "    for filename in (treatments_path):\n",
    "        treatment_paths.append(filename)\n",
    "    d[i] = pd.DataFrame(bam_paths)\n",
    "    d[i].columns = ['bam paths']\n",
    "    #d[i]['patient id'] = i\n",
    "    d[i]['peak paths'] = pd.Series(peak_paths)\n",
    "    d[i]['peak paths']= d[i]['peak paths'].fillna(method='ffill')\n",
    "    d[i]['treatments']= pd.Series(treatment_paths)\n",
    "    d[i]['treatments'] = d[i]['treatments'].str.replace('_mb', '')\n",
    "    d[i] = d[i].reindex(columns=column_names)\n",
    "    bam_paths.clear()\n",
    "    peak_paths.clear()\n",
    "    treatment_paths.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary of bed and bam paths for 19 genos ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = os.listdir('/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/19_genos_fastq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "bam_paths = []\n",
    "peak_paths = []\n",
    "treatment_paths = []\n",
    "\n",
    "\n",
    "\n",
    "column_names = [\n",
    "    #'patient id', \n",
    "                'treatments','bam paths', 'peak paths']\n",
    "\n",
    "d_add = {}\n",
    "\n",
    "#filenames = [f for f in filenames if (f.startswith(\"ERR\") and f.lower().endswith(\"1\"))]\n",
    "for i in (sample_path):\n",
    "    #print([i][0]\n",
    "    path = f\"/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/19_genos_fastq/{i}/out/treatments/*/merged_qc.bam\"\n",
    "    peak_path = f\"/iblm/netapp/data3/jjaureguy/PRJEB18997/10_genos/19_genos_fastq/{i}/out/treatments/merged.bed\"\n",
    "    for filename in glob.glob(path):\n",
    "        bam_paths.append(filename)\n",
    "    for filename in glob.glob(peak_path):\n",
    "        peak_paths.append(filename)\n",
    "    for filename in (treatments_path):\n",
    "        treatment_paths.append(filename)\n",
    "    d_add[i] = pd.DataFrame(bam_paths)\n",
    "    d_add[i].columns = ['bam paths']\n",
    "    #d[i]['patient id'] = i\n",
    "    d_add[i]['peak paths'] = pd.Series(peak_paths)\n",
    "    d_add[i]['peak paths']= d_add[i]['peak paths'].fillna(method='ffill')\n",
    "    d_add[i]['treatments']= pd.Series(treatment_paths)\n",
    "    d_add[i]['treatments'] = d_add[i]['treatments'].str.replace('_mb', '')\n",
    "    d_add[i] = d_add[i].reindex(columns=column_names)\n",
    "    bam_paths.clear()\n",
    "    peak_paths.clear()\n",
    "    treatment_paths.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_add[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(d).reset_index().drop(columns='level_1').rename(columns={'level_0': 'patient_id'})\n",
    "df_19 = pd.concat(d_add).reset_index().drop(columns='level_1').rename(columns={'level_0': 'patient_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df, df_19]\n",
    "\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_offset = 0\n",
    "bed_dfs = {}\n",
    "for (patient_id, bam_path, peak_file), pdf in tqdm(result.groupby(['patient_id','bam paths', 'peak paths'])):\n",
    "    \n",
    "    bed_df = pd.read_csv(peak_file, delimiter='\\t', \n",
    "                         header=None, names=['chrom', 'start', 'end'], \n",
    "                         dtype={'chrom':'category', 'start':int, 'end': int})\n",
    "    \n",
    "    bed_df['bam paths'] = bam_path\n",
    "    bed_df['peak paths'] = peak_path\n",
    "    bed_dfs[patient_id] = bed_df\n",
    "dataset_df = pd.concat(bed_dfs, names=['patient_id']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_df['level_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = dataset_df['bam paths'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing bam files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = result['bam paths'].unique()\n",
    "\n",
    "c = np.setxor1d(x, car)\n",
    "c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genome write read depth from bam files of df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated partial list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in tqdm(dataset_df['bam paths'].unique()):\n",
    "    fnp = Path(fn)\n",
    "    outdir = fnp.parent\n",
    "    write_read_depth(str(fnp), str(outdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest of bam files missing from previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in tqdm(c):\n",
    "    fnp = Path(fn)\n",
    "    outdir = fnp.parent\n",
    "    write_read_depth(str(fnp), str(outdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.to_csv('/iblm/netapp/data4/jjaureguy/jupyter/atac_ml_project/data_frame/df.txt', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "27823fb9faee6939bb67145c29e9f7439d6f8c9b03d9abf7d135c0d798c1cb44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
